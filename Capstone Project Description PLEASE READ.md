# Solamon-Portfolio-Projects - Capstone Project: Executive Dashboards with Expert Modeling

This capstone project will mirror one of my most valuable reports in production at Micromeritics Instrument Corporation. It delivers a high-level synopsis of operations through well-defined KPIs and intuitive drill-down capabilties. We use the report as our slide deck for weekly revenue meetings because the golden data model behind the report allows the presenter to answer almost any question that a stakeholder can have about our business.

### **Project Phases:**

  - [**Final Product:** Executive Dashboards]()
  - [**Build Phase #1:** ERP Customizations with D365 Business Central AL]()
  - [**Build Phase #2:** Orchestrating Dataflows with Power Automate and Sharepoint]()
  - [**Build Phase #3:** ETL and Data Modeling with Power Query M]()
  - [**Build Phase #4:** DAX Measures and Dashboarding in Power BI]()

  **Final Product: Executive Dashboards**

  1. Here is a direct link to the report: [SolaCorp Executive Dashboards]()
  2. The purpose of this report is to provide executive-level stakeholders and key operations personnel with insights into week-over-week manufacturing performance. The most important questions that the report answers are:

  - How much product did we ship last week?
  - How much revenue can we expect from these shipments?
  - How much was shipped but not invoiced last week?
  - How much do we plan on shipping this week (current week greenlist)?
  - How much of the previous week greenlist did not ship last week?
  - How much of our CW greenlist is on hold? How much is intercompany? Where are they shipping?
  - What was the impact of our orders/shipments on the backlog?
  - What is the composition of our backlog? How much is on hold? What are our top items?
  - What is our backlog aging? Which orders are the oldest?
  
  3. The report answers the first four of these questions in the executive summary page:

  ![Executive Summary]()
     
  4. The rest can be answered in subsequent pages with user-friendly interfaces:

  ![Inst. Shipments]()

  ![Instrument Backlog]()


  **Build Phase #1: ERP Customizations in D365 Business Central AL**
  
  1. This project uses some light ERP customization to help users coordinate the greenlist in the ERP (D365 Business Central) and lets the Shipping Department know which orders are clear to ship.
  2. I added a custom enum field called "Color Status" to the Sales Header table that makes the order number bold green when the order has been added to the greenlist using the "Toggle Include in Greenlist" action:

  ![Sales Orders]()
  
  3. The shipment number also turns bold green to allow the Shipping Department to see which orders to work on. If an order has been added to the greenlist in the last 24 hours, the "Recent Greenlist" field will let Shipping know it's new:

  ![Warehouse Shipments]()
  
  4. To achieve these customizations, I used AL code to write page extensions that set a global variable (StyleExprNo) based upon the value of the "Color Status" field:

  ![StyleExprNo]()

  5. I also added 3 actions to the page to allow users to toggle between color statuses. If you would like to see how these are written in AL code, please see [this file]().
  6. The final major feature that I added was a "Recent Greenlist" field, which takes the system current datetime and subtracts from a custom field "Added to Greenlist DateTime" to get a duration. If this duration is less than 24 hours, the field is set to "24 hr." Here is the AL code for this feature:

  ![Recent Greenlist]()


  **Build Phase #2: Orchestrating Dataflows with Power Automate and Sharepoint**
  
  1. A diagram showing the orchestration of data from D365 Business Central to Power BI is given below. Please note that this orchestration only applies for the LIVE version of this report. This capstone project references STATIC files on GitHub that were generated by redacting/randomizing information from our Sharepoint warehouse:

  ![ERP to Power BI Orchestration]()
     
  2. Most of the data for these executive dashboards starts in the ERP (D365 Business Central).
  3. We use Power BI dataflows and the Business Central API connector to read table data into staging dataflows.
  4. We use these staging dataflows as a data source for either Power BI semantic models or secondary dataflows to apply transformation steps. Using staging dataflows allows for much faster ETL.
  5. We end up with semantic models containing cleaned tables for sales, orders, backlog, current greenlist, and any necessary helper tables.
  6. We then use Power BI Report Builder to build paginated reports of these cleaned tables using the published semantic models as our data sources.
  7. After publishing these paginated reports to a premium Power BI workspace, we can use Power Automate to refresh our models on a schedule and export our paginated reports to an XLSX file.
  8. Power Automate can then update a file location in Sharepoint, which acts as an informal data warehouse. If we were dealing with larger volumes of data, we may choose to use CSV files, a MS SQL Server data warehouse, or Fabric Data Lakehouse.
  9. In the next section, I will talk about ETL from this Sharepoint warehouse and building the "golden" data model.


  **Build Phase #3: ETL and Data Modeling with Power Query M**
  
  1. When reading data from the Sharepoint warehouse into Power Query, we use the Web Contents connector (because it is significantly faster than the Sharepoint connector).
  2. Because I have already done extensive cleansing when moving data from the "bronze" layer in Business Central to the "silver" layer in the Sharepoint warehouse, the transformation steps in Power Query when building the "golden" layer are fairly simple. For the most part, I only use filtering and aggregation. However, below is an example of a more advanced method that I used:

  - Extracted a previous week (Saturday PM) backlog snapshot from the data warehouse. Added a primary key called "Order_Line," which was defined as {Order Number}_{Line Number}.
  - Added a "Ship Qty" field to a new table called "Warehouse Shipment Lines" by taking all warehouse shipments up to the end of the previous fiscal week and agregating ship quantity over the primary key (Order_Line).
  - Added an "Inv Qty" field to the "Warehouse Shipment Lines" table by taking all invoices up to the end of the previous fiscal week, aggregating invoice quantity over the primary key (Order_Line), and joining/merging this field to the "Warehouse Shipment Lines" table on the Order_Line key.
  - Subtracted "Ship Qty" minus "Inv Qty" to get a new field "Ship-Not-Inv Qty" and "Ship-Not-Inv $" then joined these fields to the PW backlog snapshot to get the list of orders from last week that were shipped but not invoiced by Customer Service. This is one of their most important KPIs and gives a more accurate picture of the Shipping Department's performance.

  3. Below are some of these principles that I kept in mind to keep the average refresh time under 2 minutes despite the large number of aggregations and joins:

  - Use normalized data to import narrow fact tables and short as possible dimension tables.
  - Filter rows, remove columns, join, and group before using transformations that will break query folding like change type, add index, or window functions.
  - Pre-aggregating tables where you don't need high granularity will give you performance when loading visuals/slicing, but it will slow down refresh times. Push these types of pre-aggregations to the silver layer.

  4. After applying these transformation steps in Power Query, we now build the relationships between the tables in the model view of Power BI:
 
  ![Golden Data Model]()
  
  5. This "golden" data model is expertly built for the following reasons:

  - All fact tables (tables with red stars) are built using star schema and are always the "many" part of a one-to-many relationship (yellow highlight). This feature is essential for having performant visuals and simple DAX.
  - Dimension tables (labeled with blue "d") are always the "one" side of a one-to-many relationship. Furthermore, I use dimension tables for slicers with single-direction filtering whenever possible.
  - I have dedicated date tables for the current backlog, historic parts backlog, historic instruments backlog, and sales/orders tables so that these dimension tables can be as short as possible.
  - Instead of using one wide, highly granular fact table, I use 7 narrow, pre-aggregated fact tables to optimize visual and slicer performance.
  - I handle a many-to-many relationship between sales/order week and fiscal week using a bridge table (labeled with green "B"). A given sales/order week can have many sales, and a given fiscal week can have many dates.


  **Build Phase #4: DAX Measures and Dashboarding in Power BI**
  
  1. 
